AIPlanner - Deep Reinforcement Learning Financial Planner
=========================================================

This document deals with AIPlanner development and the AIPlanner
website.

See README.docker for information on configuring and running the
AIPlanner api server.

Server requirements
-------------------

AIPlanner runs on Ubuntu 20.04 on Amazon EC2. For production 50G of
disk space should be plenty, but for development 100G might be desired
to enable saving multiple checkpoints from multiple training runs.

For the web server which includes the api server in a docker
container, with num_infer_jobs=0 set, a 2 core 4 thread 8G RAM
c5.xlarge instance can be used.

Training the neural network requires running about 2 c5.18xlarge spot
instances for around 20 hours. The us-east-2 region currently appears
the cheapest region for performing this training work.

For development an m5.large, or even a t3.small instance, can often be
used, provided in the first case a Ray compute cluster is configured,
or in the second case --train-seeds=1 --num-workers=0 is passed to
train_rllib.py and --train-seeds=1 is passed to api/server.py .

For deployment on the Internet the machine will need to have ports 80
and 443 open (http and https), along with 20 (ssh) for administrative
access. An elastic IP will need to be associated with the machine.
And www.<thisdomain> and <thisdomain> should be CNAMES/A records for
the elastic IP's public DNS name/address respectively.

For deployment on an Intranet only ports 443 and 20 are required.
Creating a CNAME for the private DNS name is probably also desirable.

Set up hostname
---------------

This step is optional. It helps when using EC2 to give various
instances meaningful names.

sudo bash
echo <unqualified-hostname> > /etc/hostname
hostname -F /etc/hostname
[edit] /etc/hosts
modify: 127.0.0.1 localhost <unqualified-hostname>
exit

Obtain the source
-----------------

Set up an EC2 instance, ssh in to it, and obtain the sources:

    cd ~
    git clone https://github.com/gordoni/aiplanner.git
    cd aiplanner
        # Or currently for internal development:
        #git clone git@git.assembla.com:oaa.git aiplanner
        #cd aiplanner
        #git checkout aiplanner
    git config --global user.name "<FirstName> <LastName>"
    git config --global user.email "<user@email.com>"

Obtaining additional required components
----------------------------------------

Installing additional required components on Ubuntu 20.04:

    sudo
    apt install cython3
    apt install catdoc python3-setproctitle gnuplot-nox python3-reportlab docker.io
    apt install python3-xlrd # for web/update_price.py
    apt install python3-pip python3-dev python3-boto3 # python3-dev might not be required.
    #apt install python3-mpi4py joblib # only needed if using OpenAI baselines
    pip3 install arch
        # Only needed if you wish to fit your own volatility model.
        # May require: --install-option='--no-binary'
    apt install nodejs npm
    #apt install node-typescript # No longer needed.
    pip3 install torch
    #pip3 install tensorflow # No longer used; use pytorch instead.
    apt install cmake # Needed for atari-py dependency of ray[rllib].
    pip3 install 'ray[rllib,debug]==1.1.0'
        # If run into problems with atari-py try the following:
        # RLlib depends on atari-py which used to require bazel which requires uzip to install.
        #apt install unzip
        #(cd aiplanner && ai/ray/ci/travis/install-bazel.sh) # Install bazel in /home/ubuntu/bin
        #PATH=/home/ubuntu/bin:$PATH pip3 install 'ray[rllib,debug]==1.1.0'
    pip3 install pyyaml svglib
    exit

The standard version of Ray works unmodified, but if you need to use a
locally modified version proceed as follows (the current local version
of Ray is the official Ray 0.8.5 release and will also need to be
updated):

    # Hack Ray to use our version of RLlib, not the default version:
    RAY=`pip3 show ray | grep Location | awk '{print $2}'`/ray
    mv $RAY/rllib $RAY/rllib.old
    ln -s ~/aiplanner/ai/ray//rllib $RAY/rllib
    # Note, you may need to disable this link if upgrading Ray to
    # prevent pip clobbering the repository.

If plan to deploy the web UI then install the required components for
the Angular website:

    sudo npm install -g @angular/cli

Cache the interest rate data
----------------------------

Failing to perform this step will result in the full historical yield
curves getting downloaded from the Fed every time a cluster node
starts up.

    mkdir ~/aiplanner-data
    ~/aiplanner/ai/web/cron.daily

Also set the fair market price and volatility levels:

    ~/aiplanner/ai/web/cron.weekly

Compile the code
----------------

Much of the Python code can be compiled with Cython.

Performing this step result in a financial model that runs roughly 5
times faster.

    cd ~/aiplanner/spia
    ./cythonize

    cd ~/aiplanner/ai
    ./cythonize

Setting up the Ray compute cluster
----------------------------------

This step is optional.

A Ray compute cluster can be used to train the neural network models
on cheaper spot instances rather than on demand instances.

The cluster works most effectively if the head node and worker nodes
are of similar instance families. Otherwise jobs will execute at
uneven rates.

Create a cluster config file:

    cd ~/aiplanner/ai
    cp cluster-example.yaml cluster.yaml
    [edit] cluster.yaml
        # Alter available_node_types:default_node parameters
        # node_config:InstanceType and resources:CPU if desired.
        # For maximum speed specify CPU:<num_cores>.
        #     c5.18xlarge has 36 cores, but setting CPU:71 is
        #     probably better for training as each core has 2 threads
        #     and typically use 2 instances to train 7 models with 10
        #     seeds each and with 1 workers and 1 driver in parallel.
        # Other changes as desired.

Rather than have separate cluster head and worker nodes, we are going
to have the AIPlanner host act as the head node. This allows the web
server and jobs that use Ray to share result data directories, but it
requires a little fancy footwork.

First create the cluster IAM role, ssh key pair, and security group.

The easy way to do this is risky as it involves giving full access to
AWS to untrusted code. This may be acceptable if you don't have any
other AWS resources.

If you wish to use the easy risky way, place your AWS credentials in
~/.aws/credentials as described in the boto3 docs.

-----
[default]
aws_access_key_id=<aws_access_key_id>
aws_secret_access_key=<aws_secret_access_key>
-----

    cd ~/aiplanner/ai
    ray up cluster.yaml
    <answer NO to "create new cluster">

This will create the ssh key pair, IAM role, and network security
group.

If you wish to use the difficult but secure method, first use EC2 to
generate an EC2 key pair called ray-autoscaler_us-east-2.pem, and save
it to the .ssh directory of the AIPlanner host. Then create a security
group tagged ray-autoscaler-default that allows all inbound traffic
from itself, inbound ssh from the Internet if desired, and all
outbound traffic. Finally generate an EC2 IAM role named
ray-autoscaler-v1 with the following inline IAM policy attached (in
which sg-AAA is replaced by the ray-autoscaler-default security group;
use multiple security-group lines if have more than one region):

{"Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AllowRunInstancesTagged",
            "Effect": "Allow",
            "Action": [
                "ec2:RunInstances"
            ],
            "Resource": "arn:aws:ec2:*:*:instance/*",
            "Condition": {
                "ForAnyValue:StringLike": {
                    "aws:RequestTag/Name": "ray-default-*"
                }
            }
        },
        {
            "Sid": "AllowRunInstances",
            "Effect": "Allow",
            "Action": "ec2:RunInstances",
            "Resource": [
                "arn:aws:ec2:*:*:subnet/*",
                "arn:aws:ec2:*:*:security-group/sg-AAA",
                "arn:aws:ec2:*:*:volume/*",
                "arn:aws:ec2:*:*:network-interface/*",
                "arn:aws:ec2:*::image/*",
                "arn:aws:ec2:*:*:key-pair/*"
            ]
        },
        {
            "Sid": "AllowCreateTagsWhenLaunching",
            "Effect": "Allow",
            "Action": [
                "ec2:CreateTags"
            ],
            "Resource": [
                "arn:aws:ec2:*:*:instance/*",
                "arn:aws:ec2:*:*:volume/*"
            ],
            "Condition": {
                "StringEquals": {
                    "ec2:CreateAction": "RunInstances"
                }
            }
        },
        {
            "Sid": "AllowActionTaggedInstances",
            "Effect": "Allow",
            "Action": [
                "ec2:TerminateInstances",
                "ec2:CreateTags"
            ],
            "Resource": "arn:aws:ec2:*:*:instance/*",
            "Condition": {
                "StringLike": {
                    "ec2:ResourceTag/Name": "ray-default-*"
                }
            }
        },
        {
            "Sid": "AllowDescribe",
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeInstances",
                "ec2:DescribeSecurityGroups",
                "ec2:DescribeSubnets"
            ],
            "Resource": "*"
        },
        {
            "Sid": "AllowGetInstanceProfile",
            "Effect": "Allow",
            "Action": [
                "iam:GetInstanceProfile"
            ],
            "Resource": "arn:aws:iam::*:instance-profile/ray-autoscaler-v1"
        }
    ]
}

You may wish to further harden things by restricting the AWS image
that can be used.

Either way the required AWS resources have now been
created. Additional work is required so that the AIPlanner host can
act as the head node.

From the EC2 console perform actions on this host setting the instance
setting IAM role to ray-autoscaler-v1 and networking change security
groups to add ray-autoscaler-default.

For syncing worker node result directories create a link:

    ln -s ~/.ssh/ray-autoscaler_us-east-2.pem ~/ray_bootstrap_key.pem

To the "worker_nodes" section of cluster.yaml add:

----
    SubnetIds:
        - subnet-AAA
    SecurityGroupIds:
        - sg-AAA
----

where the subnet is the subnet in which you wish to create worker
nodes, and the security group identifies ray-autoscaler-default.

If desired, the cluster can now be manually started using:

    cd ~/aiplanner/ai
    ray start --head --redis-port=6379 --autoscaling-config=cluster.yaml

Worker nodes are created when needed, and terminated if idle for a
while.

The cluster can be torn down using:

    cd ~/aiplanner/ai
    ray stop
        # Stops Ray on the head node.
    ray down cluster.yaml
        # Terminates any running worker nodes.

A dedicated worker AMI image can be created to speed up the cluster
start up process. To do this temporarily set min_workers to 1 in
cluster.yaml and then start up the head node as above. From the EC2
console select the instance and perform action - image - create image.
Update the worker_nodes ImageId with the newly create AMI id.

Setting up environment variables
--------------------------------

If you don't want to use a Ray compute cluster during training:

    export RAY_AUTOSCALER=False

If you are using a Ray compute cluster during training then you may
want to specify:

    export RAY_AUTOSCALER_USE_HEAD=True

This will allow training on the head node. The reason you may not want
to do this is the head node will likely be slower than the worker
nodes because it is also running the Ray monitor process. This slows
the training of multiple neural networks in parallel.

Testing a few things out
------------------------

You might wish to test a few low level commands are working.

    cd ~aiplanner/ai
    source helpers.bash # Setup routines and environment variables.

Local model training:

     RAY_AUTOSCALER=False ./train_rllib.py --train-seeds=1 --train-num-workers=0 --train-batch-size=10000 --train-num-timesteps=20000 -c aiplanner-scenario.txt -c aiplanner-scenario-train.txt
    # Ray checkpoints saved in aiplanner.tf

Cluster based training:

    start_ray_if_needed
    ./train_rllib.py --address=localhost:6379 --train-seeds=1 --train-num-workers=2 --train-batch-size=10000 --train-num-timesteps=20000 -c aiplanner-scenario.txt -c aiplanner-scenario-train.txt
    # Ray checkpoints saved in aiplanner.tf
    ray stop
    ray down cluster.yaml

Cluster based checkpoint evaluation (optional):

    start_ray_if_needed
    ./eval_model.py --address=localhost:6379 --train-seeds=1 --eval-num-timesteps=10000 -c aiplanner-scenario.txt -c aiplanner-scenario-single-eval.txt
    # Results saved in results/seed_0
    ray stop
    ray down cluster.yaml

Convert Ray checkpoints to export format (only for TensorFlow
non-eager):

    ./export_model.py --train-seeds=1
    # Tensorflow model saved to aiplanner.tf

But AIPlaner doesn't current work with TensorFlow non-eager.

Ray 1.1.0 is unable to export TensorFlow eager and PyTorch based
models, consequently all evaluation is currently based on checkpoint
models. TensorFlow based checkpoint models take around 3 seconds of
cpu time to load (per seed), and as a result the model server would
take several minutes to start up. Fortunately PyTorch based checkpoint
models only take 0.2 seconds of cpu time to load.

Local TensorFlow model or checkpoint evaluation:

    # Saved tensorflow model takes precedence over Ray checkpoints
    ./eval_model.py --train-seeds=1 --eval-num-timesteps=10000 -c aiplanner-scenario.txt -c aiplanner-scenario-single-eval.txt
    # Results saved in results/seed_0

Fit the equity volatility model
-------------------------------

A bootstrapped equity return model with GJR-GARCH volatility is
typically used. The model has already been fitted, but if you wish to
fit it yourself perform the following steps.

    cd ~/aiplanner
    mkdir -p data/private/ticker
    [download the max time period ticker data for ^SP500TR and ^GSPC
     from https://finance.yahoo.com and store it in the just created
     directory as ^SP500TR.csv and ^GSPC.csv ]
    cd ~/aiplanner/ai/equity_model
    ./run
    cp standardized_residuals.csv ../../data/public
    [and use the just determined equity_model.out --stocks-*
     parameters when training and evaluating]

Training the generic neural network models
------------------------------------------

Unless you plan on serving pre-trained neural network models you will
need to train the neural network models.

Training the neural network models (will take arond 20 hours on about
2 InstanceType c5.18xlarge instances with resources CPU: 71; edit
cluster.yaml accordingly; many smallish instances may be preferable to
a few very large instances as Amazon sometimes seems more likely to
reclaim very large spot instances slowing the training process):

    cd ~/aiplanner/ai/api
    nohup ./train > /tmp/train.out &

A presumed bug in Ray 0.7.3 meant that completion of training of the
first training run kills all the other training workers. It is fixed
in Ray 0.8.5, but if it recurs, it can be worked around by specifying:

    nohup ./train --train-num-workers=0 --num-cpu=1 > /tmp/train.out &

but it increases the elapsed training time (to around 100 hours on a
single c5.24xlarge instance).

Training logs will be written to /tmp/train* . Progress can be
monitored using a command similar to:

    tail -f /tmp/train* | grep seed

Note that any AIPlanner warnings from a single model's training get
duplicated across all log files.

The trained models will be placed in ~/aiplanner-data/models.new .

To quickly generate dummy untrained models that can be used by the api
server:

    cd ~/aiplanner/ai/api
    ./train --train-batch-size=1000 --train-num-timesteps=1000 --train-num-workers=0

By default trained models will be served without any adjustments for
model biases. If you know the model is biased it is possible to apply
corrections when serving that attempt to overcome the model bias:

    [edit] ~/aiplanner-data/models.new/models-adjust.json

See ~/aiplanner/ai/validation/run-ai6-bias for an example script for
assessing bias. Correcting for bias only seems to be important for
RRA=6. See ~/aiplanner/ai/models-adjust-example.json for an example of
how to represent the bias adjustments. Note that the same bias
adjustment is applied to all uses of a particular model.

The models should then be installed:

    cd ~/aiplanner-data
    rm -rf models.old # Delete prior model backups if required.
    mv models models.old # Save prior models if any as a backup.
    mv models.new models

Generating required assets for the UI website (images)
------------------------------------------------------

Both of the UI website sections can be skipped if you only wish to run
the AIPlanner API server.

Generate the images required for the UI website (takes around 15
minutes):

    cd ~/aiplanner/ai/angular/src/assets.generate
    ./run

Build the UI website files
--------------------------

Angular provides a single webpage app for interacting with the models.

Install the Angular builder:

    cd ~/aiplanner/ai/angular
    npm ci

Angular downloads 500MB of node modules in order to build the website.
These files would slow down the process of synchronizing source trees
when starting up a new Ray cluster node. Therefore if Ray is used it
is recommended that the node modules be stored outside the source
tree:

    cd ~/aiplanner/ai/angular
    mv node_modules ~/
    ln -s ~/node_modules ~/aiplanner/ai/angular/

Build the website:

    cd ~/aiplanner/ai/angular
    ng build --prod

Install the website:

    cd ~/aiplanner/ai/angular
    ./install

Set up the web server
---------------------

Even if only running the API server, you may wish to set up a
webserver. In this case the webserver will be responsible for proxying
/api TLS requests on port 443 to the API server. If you don't
intend to use TLS internally for talking to the API server you can
skip this section and have applications connect to the API server
directly without encryption, either on the default port (3000), or the
http port (80) by adding the argument "--port 80" when installing the
aiplanner-apiserver.service file later.

    sudo apt install apache2
    sudo a2enmod ssl proxy proxy_http
    sudo systemctl restart apache2

    Select appropriate <site-ssl.conf> file:
        sudo cp aiplanner/ai/web/sites-examples/<sample-site-ssl.conf> /etc/apache2/sites-available/<site-ssl.conf>
    Edit site.conf appropriately:
        sudo <edit> /etc/apache2/sites-available/<site-ssl.conf>

    Repeat installing <site.conf> or <site-ssl.conf> for additional
    sites to be served if desired, so that http://www.<site>/,
    http://<site>/, and https://<site>/ will all point to
    https://www.<site>/.

    Enable each plain <site.conf> (but not the <site-ssl.conf>s):
        sudo a2ensite <site>

    sudo apt install python3-certbot-apache
    sudo certbot --apache certonly
        # Add "--domains <thisdomain>" to force a particular domain.
    Email: <you@yourdomain>
    Domains: 
        If serving for www.<thisdomain> then typically: <thisdomain>, www.<thisdomain>
        If serving for a particular host: <thishost.thisdomain>

    Enable each <site-ssl.conf>:
        sudo a2ensite <site-ssl>

    sudo systemctl reload apache2

Building an api server docker container
---------------------------------------

The api server can be distributed as a docker container.

    cd ~/aiplanner/ai
    sudo docker build -t aiplanner-base-deps docker/base-deps
    tar cf docker/apiserver/dotspia.tar -C ~/ .spia
    (cd ~/aiplanner-data/models; ls | grep -v .tf; ls */params.txt ) > docker/apiserver/MANIFEST.models
    (cd ~/aiplanner-data/models; for d in *.tf/seed_*_*; do echo $d/checkpoint_`ls $d | sed 's/checkpoint_//' | sort -n | tail -n 1`; done) >> docker/apiserver/MANIFEST.models
    tar zcf docker/apiserver/models.tgz -C ~/aiplanner-data/models `cat docker/apiserver/MANIFEST.models`
    tar zcf docker/apiserver/webroot.tgz -C ~/aiplanner-data/webroot .
    git rev-parse HEAD > docker/apiserver/git-rev
    rm -rf docker/apiserver/aiplanner
    mkdir -p docker/apiserver/aiplanner
    (cd ..; git archive `cat ai/docker/apiserver/git-rev`) | tar xf - -C docker/apiserver/aiplanner `cat docker/apiserver/MANIFEST.aiplanner`
    (cd docker/apiserver/aiplanner/spia; ./cythonize.py)
    (cd docker/apiserver/aiplanner/ai; ./cythonize.py)
    sudo docker build -t aiplanner-apiserver docker/apiserver
    rm -r docker/apiserver/{dotspia.tar,MANIFEST.models,models.tgz,webroot.tgz,git-rev,aiplanner}

Test it out:

    touch $HOME/aiplanner-data/subscribe.txt # Create an empty AIPlanner email address interest list that will exist outside the container.
    sudo docker run --rm -ti -p 127.0.0.1:3000:3000 -v $PWD/aiplanner-docker-development.yaml:/aiplanner.yaml -v $HOME/aiplanner-data/subscribe.txt:/home/aiplanner/aiplanner-data/subscribe.txt aiplanner-apiserver

Test the image thoroughly. If everything looks good publish the image
to a private repo:

    sudo docker image tag aiplanner-apiserver <dockerhubid>/aiplanner-apiserver:<version>
    sudo docker image tag aiplanner-apiserver <dockerhubid>/aiplanner-apiserver:latest
    docker login
    sudo docker push <dockerhubid>/aiplanner-apiserver:<version>
    sudo docker push <dockerhubid>/aiplanner-apiserver:latest
    docker logout

Set up the aiplanner system configuration file
----------------------------------------------

    cd ~/aiplanner/ai
    cp aiplanner-example.yaml ~/aiplanner-data/aiplanner.yaml
    sudo [edit] ~/aiplanner-data/aiplanner.yaml
        # Possibly change port to 80 if running the api server within an Intranet
        # Set num_infer_jobs to 0 if only running the website
        # Set notify_email to the address that will send notifications notify@<thisdomain>
        # Set admin_email to the address that will receive notifications <you@yourdomain>

Run the api server in production
--------------------------------

    docker login
    sudo docker pull <dockerhubid>/aiplanner-apiserver
    docker logout
    sudo docker ps
    sudo docker stop <containerid>
    sudo docker run -d -t --restart=unless-stopped -p 127.0.0.1:3000:3000 -v ~/aiplanner-data/aiplanner.yaml:/aiplanner.yaml <dockerhubid>/aiplanner-apiserver

Setup the backend web server
----------------------------

The backend web server responds to requests for web related services
from the website. Currently this only comprises handling email
subscription requests.

Set up the systemd config:

    cd ~/aiplanner/ai
    sudo cp aiplanner-webserver.service /etc/systemd/system/
    sudo systemctl enable aiplanner-webserver

Start the service up:

    sudo systemctl start aiplanner-webserver

Setting up cron
---------------

Install crontabs to perform a daily refresh of the interest rate yield
curves, and a weekly refresh of the fair market price and volatility.

    cd ~/aiplanner/ai
    cat web/aiplanner.crontab
    crontab -e

----
# m h  dom mon dow   command
MAILTO=<you@yourdomain>
<place output from "cat web/aiplanner.crontab" here>
----

Seting up log rotation
----------------------

    sudo chmod g-w ~/aiplanner-data/ # Required by logrotate.

    cd ~/aiplanner/ai
    sudo cp web/aiplanner.logrotate /etc/logrotate.d/aiplanner

Installing an outbound mailer
-----------------------------

A mail transfer agent is only required to provide system notification
messages to the administrator. These consist of subscription
notifications generated by users of the web UI.

    sudo apt install postfix
        # Any choices here are fine as we are about to reconfigure.
    sudo dpkg-reconfigure postfix
        Config type: Internet Site
        Site: <thisdomain>
        Root mail: <you@yourdomain>
        Accept mail for: <thisdomain>
        Synchronous updates: no
        Local networks: <leave as is>
        Mailbox size limit: 0
        Address extension: +
        Internet protocols: all
    sudo <edit> /etc/postfix/main.cf
        Change: myhostname = <...>
        To: myhostname = <thisdomain>
    sudo /etc/init.d/postfix reload

You may also need to add an SPF record for the site so messages from
it don't end up in the spam folder. Something like:

    Type: TXT
    Host: @
    Value: v=spf1 ip4:18.206.41.110 ~all
    TTL: 3600

Inbound mail flow isn't really required, but if you wish to enable
it follow the remaining steps.

You can add a MX record so the server can receive and forward bounce
messages addressed to root. Something like:

    Type: MX
    Host: @
    Priority: 1
    Value: ec2-18-206-41-110.compute-1.amazonaws.com
    TTL: 3600

Consult your domain provider for details of how to do these steps.

Finally you can create a firewall rule to allow inbound SMTP
connections on port 25. Consult your service provider for details.

Performing site health checks
-----------------------------

For a production website or api server Uptime robot, or some other
service should be used to perform periodic HTTPS GET /api/healthcheck
requests to port 443 of the production server.  After about 1 second,
the string "OK" should be returned as the body to indicate the system
appears to be operating normally, any other result indicates a
problem.

Development
-----------

The api server has several functions:
    - respond to the Angular code proxying to it to handle HTTP /api
      and /web requests
    - respond to HTTP /api/healthcheck requests
    - serve web content from ~/aiplaner-data/webroot for any other
      urls
    - purge old results from the directory ~/aiplanner-data/results

The api server can be run in development as follows:

    cd ~/aiplanner/ai
    source setenv # Setup environment variables.
    # First create a default market-data.json file
    cp docker/apiserver/market-data.json ~/aiplanner-data/
    # Stop any running apiserver container
    sudo docker ps
    sudo docker stop <containerid>
    # Then start up the development apiserver
    api/server.py --host=localhost

On the same machine in another shell window run the Angular server:

    cd ~/aiplanner/ai/angular
    ng serve --host 0.0.0.0 --public-host <thishost.thisdomain>

Connect to the front end using a browser (need to open port 4200 of
development machine firewall unless doing development on localhost):

    http://<thishost.thisdomain>:4200/

Currently at Angular version 11.  Should upgrade to the final release
of the current major version before upgrading the to next major
version, and upgrade one major version number at a time. To upgrade
Angular:

    cd ~/aiplaner/ai/angular
    npm install @schematics/angular@<major_version> @angular-devkit/core@<major_version> @angular-devkit/schematics@<major_version>
    git commit packagejson package-lock.json
    ng update @angular/core@<major_version> @angular/cli@<major_version>
    git commit -a
    ng update @angular/material@<major_version>
    git commit -a
