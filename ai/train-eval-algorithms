#!/bin/bash

# AIPlanner - Deep Learning Financial Planner
# Copyright (C) 2018-2019 Gordon Irlam
#
# All rights reserved. This program may not be used, copied, modified,
# or redistributed without permission.
#
# This program is distributed WITHOUT ANY WARRANTY; without even the
# implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR
# PURPOSE.

AIPLANNER_HOME=${AIPLANNER_HOME:-$HOME/aiplanner}

source $AIPLANNER_HOME/ai/helpers.bash

mkdir -p ~/aiplanner-data/train-results
cd ~/aiplanner-data/train-results

PARALLEL=True
TRAINING=generic
UNIT=single
STAGE=retired
SPIAS=none
GAMMA=3
TRAIN_ARGS="$TRAIN_ARGS --train-num-timesteps=1000000"
EVALUATOR="$EVALUATOR --master-consume-clip=5000"
    # We specify a consume clip so that we don't overlook algorithms that assume it from training.
    # Should later confirm it isn't needed by running tests on the chosen algorithm without it.
for ALGORITHM in PPO ppo1 td3 sac trpo ppo ddpg vpg; do

    mkdir -p $ALGORITHM
    cd $ALGORITHM

    train_eval $TRAINING $UNIT $STAGE $SPIAS $GAMMA

    cd ..

done

echo `date` Done

# Notes.
#
# Single with no nominal spias:
#     RLLib PPO and PPO.baselines outperforms baselines ppo1 (not sure why).
#     RLlib PPO outperforms all other continuous action space RLlib algorithms:
#         A2C - gave 0/100% stocks/bonds allocation; likely a bug.
#         A3C - not tested; A3C asynchronous variant (reportedly inferior).
#         PG - presumed inferior.
#         DDPG - ran too slowly (50,000 timesteps per CPU hour without GPU).
#         APPO - PPO asychronous variant; enables faser PPO training.
#         APEX-DDPG - not tested; DDPG variant.
#         ES, ARS - not tested; need rollouts, which would be very expensive.
#         MARWEL - not tested; offline algorithm.
#         IMPALA - inferior to PPO; tested batch size 500/200000, lr 5e-5/5e-6
#     Baselines ppo1 outperforms all spinnup algorithms (td3, sac, trpo, ppo, ddpg, and vpg).
#
# PPO with issue 50 fixed:
#     Very small improvement in performance.
