#!/bin/bash

# AIPlanner - Deep Learning Financial Planner
# Copyright (C) 2018 Gordon Irlam
#
# All rights reserved. This program may not be used, copied, modified,
# or redistributed without permission.
#
# This program is distributed WITHOUT ANY WARRANTY; without even the
# implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR
# PURPOSE.

AIPLANNER_HOME=${AIPLANNER_HOME:-$HOME/aiplanner}

source $AIPLANNER_HOME/ai/helpers.bash

mkdir -p results
cd results

PARALLEL=Jobs
for ALGORITHM in ppo1 td3 sac trpo ppo ddpg vpg; do

    mkdir -p $ALGORITHM
    cd $ALGORITHM

    if [ $ALGORITHM == ppo1 ]; then
        TRAINER="$AI_DIR/train_ppo1.py"
    else
        TRAINER="$AI_DIR/train_spinup.py --train-algorithm=$ALGORITHM"
    fi
    TRAINER="$TRAINER --train-num-timesteps=1000000"
    EVALUATOR="$AI_DIR/eval_model.py --master-consume-clip=5000"

    echo `date` Training $ALGORITHM

    train age_start50-age_retirement65-defined_benefits16e3-tax_free5e5 "-c $SINGLE_EVAL_FILE --master-p-tax-free=5e5"
    train age_start50-age_retirement65-defined_benefits16e3-tax_deferred5e5 "-c $SINGLE_EVAL_FILE --master-p-tax-deferred=5e5"
    train age_start50-age_retirement65-defined_benefits16e3-taxable_stocks5e5 "-c $SINGLE_EVAL_FILE --master-p-taxable-stocks=5e5"
    train retired65-defined_benefits16e3-tax_free5e5 "-c $SINGLE_EVAL_FILE --master-age-start=65 --master-p-tax-free=5e5"
    train gamma3 '--master-gamma=3'

    wait

    echo `date` Evaluating $ALGORITHM

    evaluate age_start50-age_retirement65-defined_benefits16e3-tax_free5e5 specific "-c $SINGLE_EVAL_FILE --master-p-tax-free=5e5"
    evaluate age_start50-age_retirement65-defined_benefits16e3-tax_deferred5e5 specific "-c $SINGLE_EVAL_FILE --master-p-tax-deferred=5e5"
    evaluate age_start50-age_retirement65-defined_benefits16e3-taxable_stocks5e5 specific "-c $SINGLE_EVAL_FILE --master-p-taxable-stocks=5e5"
    evaluate retired65-defined_benefits16e3-tax_free5e5 specific "-c $SINGLE_EVAL_FILE --master-age-start=65 --master-p-tax-free=5e5"

    evaluate gamma3 age_start50-defined_benefits16e3-tax_free5e5 "-c $SINGLE_EVAL_FILE --master-gamma=3 --master-p-tax-free=5e5"
    evaluate gamma3 age_start50-defined_benefits16e3-tax_deferred5e5 "-c $SINGLE_EVAL_FILE --master-gamma=3 --master-p-tax-deferred=5e5"
    evaluate gamma3 age_start50-defined_benefits16e3-taxable_stocks5e5 "-c $SINGLE_EVAL_FILE --master-gamma=3 --master-p-taxable-stocks=5e5"
    evaluate gamma3 retired65-defined_benefits16e3-tax_free5e5 "-c $SINGLE_EVAL_FILE --master-gamma=3 --master-age-start=65 --master-p-tax-free=5e5"

    wait

    cd ..

done

echo `date` Done

# Notes on tweaks that have been tried out:
#
# --train-single-num-timesteps=500000
#    Produced 0.0% better generic results than 1m timesteps both pposgd_simple.learn(optim_batchsize=64).
#    SUGGESTS COULD CUT TRAINING TIME IN HALF BUT MIGHT NOT WORK FOR MORE COMPLEX ENVIRONMENTS SUCH AS SPIAS.
#
# --train-single-num-timesteps=2000000
#    Produced 0.7% worse generic results than 1m timesteps both pposgd_simple.learn(optim_batchsize=64).
#
# --train-hidden-layer-size=128
#    Produced 0.2% worse generic results than hidden layer size 64 both pposgd_simple.learn(optim_batchsize=64).
#
# --train-hidden-layer-size=128 --train-single-num-timesteps=2000000
#    Produced 0.6% worse generic results than 1m timesteps and hidden layer size 64 both pposgd_simple.learn(optim_batchsize=64).
#
# pposgd_simple.learn(lam=0.98)
# pposgd_simple.learn(lam=1)
#    Produced 0.2% better and 0.1% worse generic results than lam=0.95 all pposgd_simple.learn(optim_batchsize=64).
#
# pposgd_simple.learn(optim_batchsize=128)
# pposgd_simple.learn(optim_batchsize=256)
#    Produced 0.3% better and 0.3% generic results than optim_batchsize=64.
#
# pposgd_simple.learn(adam_epsilon=1e-6)
#    Produced 0.1% worse generic results than adam_epsilon=1e-5 default both pposgd_simple.learn(optim_batchsize=64).
#
# pposgd_simple.learn(optim_epochs=20)
#    Produced 0.4% worse generic results than optim_epochs=10 both pposgd_simple.learn(optim_batchsize=64).
#
# pposgd_simple.learn(optim_batchsize=128, optim_stepsize=3e-5)
#    Produduced 0.4% worse generic results than optim_batchsize=128, optim_stepsize=3e-4.
#
# pposgd_simple.learn(optim_batchsize=128, optim_stepsize=1e-4)
#    Produduced 0.2% worse generic results than optim_batchsize=128, optim_stepsize=3e-4.
#
# --train-num-hidden-layers=3
#    Produced 0.4% worse generic results than 2 hidden layers both pposgd_simple.learn(optim_batchsize=128)
