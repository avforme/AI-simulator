#!/bin/bash

# AIPlanner - Deep Learning Financial Planner
# Copyright (C) 2018-2019 Gordon Irlam
#
# All rights reserved. This program may not be used, copied, modified,
# or redistributed without permission.
#
# This program is distributed WITHOUT ANY WARRANTY; without even the
# implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR
# PURPOSE.

AIPLANNER_HOME=${AIPLANNER_HOME:-$HOME/aiplanner}

source $AIPLANNER_HOME/ai/helpers.bash

mkdir -p ~/aiplanner-data/train-results
cd ~/aiplanner-data/train-results

PARALLEL=Jobs
SCENARIO_ARGS=""
#SCENARIO_ARGS="$SCENARIO_ARGS --master-stocks-model=iid --master-stocks-return=0.065 --master-stocks-volatility=0.174 --master-no-real-bonds --master-no-nominal-bonds --master-iid-bonds --master-iid-bonds-return=0.010 --master-iid-bonds-volatility=0.110 --master-no-bills --master-no-returns-standard-error --master-tax --master-static-bonds --master-fixed-real-bonds-rate=0.01 --master-fixed-nominal-bonds-rate=0.03 --master-real-short-rate-type=current --master-inflation-short-rate-type=current"
    # Required for validation training and evaluation.
    # Validation wealth is non-taxable, but train with taxable model to incorporate impact of tax model.
    # Real value of taxable basis decreases 2% per year due to inflation. Should not matter for validation.
TRAINING_ARGS=""
EVALUATE_ARGS=""
#EVALUATE_ARGS="$EVALUATE_ARGS --master-no-income-preretirement-taxable"
    # Additional requirement for pre-retirement validation evaluation.
EPISODE=generic
TRAINING=generic
UNIT=single
STAGE=retired
SPIAS=none
GAMMA=6
TIMESTEPS=50000000
SAVESTEPS=2000000
EVALSTEPS=4000000
BATCH_SIZE=200000

TRAINING_ARGS="$TRAINING_ARGS --train-num-timesteps=$TIMESTEPS"

make_train_dir () {

    for TD in "$@"; do
        mkdir -p $TD
        cd $TD
        for FILE in ../../$EPISODE/$TD/{seed_*,params.txt}; do
            ln -s $FILE 2> /dev/null
        done
        cd ..
    done
}

mkdir -p $EPISODE
cd $EPISODE
train_scenarios $TRAINING $UNIT $STAGE $SPIAS $GAMMA "$SCENARIO_ARGS $TRAINING_ARGS --train-batch-size=$BATCH_SIZE --train-save-frequency=$SAVESTEPS"
cd ..
wait
#exit
TS=$EVALSTEPS
#TS=$TIMESTEPS
while [ $TS -le $TIMESTEPS ]; do
    mkdir -p $EPISODE-timesteps$TS
    cd $EPISODE-timesteps$TS
    if [ "$SPIAS" = none ]; then
        SPIAS_TAG=no_spias
    else
        SPIAS_TAG=spias
    fi
    if [ $TRAINING = specific ]; then
        make_train_dir aiplanner-gamma$GAMMA-retired65-guaranteed_income20e3-tax_free{2e5,5e5,1e6,2e6}.tf
    elif [ $TRAINING = specific-p-type ]; then
        make_train_dir aiplanner-gamma$GAMMA-retired65-guaranteed_income20e3-${tax_free,tax_deferred,taxable}2e6.tf
    elif [ $TRAINING = specific-spias ]; then
        make_train_dir aiplanner-gamma$GAMMA-spias{65,75,85}-retired65-guaranteed_income20e3-tax_free{1e6,2e6}.tf
    elif [ $TRAINING = specific-spias-le ]; then
        make_train_dir aiplanner-gamma$GAMMA-spias-retired65-le{-5,0,5}-guaranteed_income20e3-tax_free2e6.tf
    elif [ $TRAINING = preretirement-gi ]; then
        make_train_dir aiplanner-preretirement-gamma$GAMMA-gi_fraction{0.03_0.1,0.1_0.3,0.3_1.0}.tf
    elif [ $TRAINING = retired-gi ]; then
        make_train_dir aiplanner-retired-gamma$GAMMA-gi_fraction{0.03_0.1,0.1_0.3,0.3_1.0}.tf
    elif [ $TRAINING = preretirement-le ]; then
        make_train_dir aiplanner.preretirement-gamma$GAMMA-le_additional{-2.5_-0.5,-0.5_1.5,1.5_3.5,3.5_5.5}.tf
    elif [ $TRAINING = retired-le ]; then
        make_train_dir aiplanner-retired-gamma$GAMMA-le_additional{-2.5_-0.5,-0.5_1.5,1.5_3.5,3.5_5.5}.tf
    elif [ $TRAINING = generic ]; then
        make_train_dir aiplanner-$STAGE-$SPIAS_TAG-gamma$GAMMA.tf
    fi
    CP=`expr $TS / $BATCH_SIZE`
    eval_scenarios $TRAINING $UNIT $STAGE $SPIAS $GAMMA "$SCENARIO_ARGS $EVALUATE_ARGS --checkpoint-name=checkpoint_$CP"
    #eval_scenarios $TRAINING $UNIT $STAGE $SPIAS $GAMMA "$SCENARIO_ARGS $EVALUATE_ARGS --checkpoint-name=checkpoint_$CP --ensemble"
    cd ..
    TS=`expr $TS + $EVALSTEPS`
done
wait

# This following code won't work as restore is broken and changed experiment parameters are currently ignored by tune on restore.
# NATS=1500000
# ANNEALS="500000 1000000 1500000 2000000"
# for ANNEAL in $ANNEALS; do
#     EPISODE="$UNIT-$SPIAS-gamma$GAMMA-nonanneal$NATS-anneal$ANNEAL"
#     mkdir -p $EPISODE
#     cd $EPISODE
#     CP=`expr $NATS / 4000`
#     TS=`expr $NATS + $ANNEAL`
#     train_scenarios $TRAINING $UNIT $STAGE $SPIAS $GAMMA "$SCENARIO_ARGS --train-restore-dir=../single-no_spias-gamma6-nonanneal/aiplanner.gamma6.tf --train-restore-checkpoint-name=checkpoint_$CP --train-num-timesteps=$TS --train-anneal-num-timesteps=$ANNEAL"
#     cd ..
# done
# wait
# for ANNEAL in $ANNEALS; do
#     EPISODE="$UNIT-$SPIAS-gamma$GAMMA-nonanneal$NATS-anneal$ANNEAL"
#     cd $EPISODE
#     eval_scenarios $TRAINING $UNIT $STAGE $SPIAS $GAMMA "$SCENARIO_ARGS"
#     cd ..
# done
# wait

# TS=2000000
# ANNEALS="0 100000 200000 400000"
# for ANNEAL in $ANNEALS; do
#     EPISODE="$UNIT-$SPIAS-gamma$GAMMA-anneal$ANNEAL-timesteps$TS"
#     mkdir -p $EPISODE
#     cd $EPISODE
#     train_scenarios $TRAINING $UNIT $STAGE $SPIAS $GAMMA "$SCENARIO_ARGS --train-anneal-num-timesteps=$ANNEAL --train-num-timesteps=$TS"
#     cd ..
# done
# wait
# for ANNEAL in $ANNEALS; do
#     EPISODE="$UNIT-$SPIAS-gamma$GAMMA-anneal$ANNEAL-timesteps$TS"
#     cd $EPISODE
#     eval_scenarios $TRAINING $UNIT $STAGE $SPIAS $GAMMA "$SCENARIO_ARGS"
#     cd ..
# done
# wait

echo `date` Done

exit

# PPO examining 4 scenarios (with 2m timestep evaluations):
#   batch_size=100000, entropy_coeff=1e-3, kl_target=1, optimizer_step_size=5e-5, optimizer_epochs=30, minibatch_size=128, lambda=1, anneal_num_timesteps=0
#   model was prior to tuneable stocks curvature
# timesteps,mean CE,CE stderr (typically 20 experiments)
# gamma=6 specific 2.5e5 single no-spias:
1000000,27141.532122,11.211862
2000000,27206.079839,4.808965
3000000,27232.146407,5.596441
4000000,27218.473246,7.722734
5000000,27226.940843,4.746282
6000000,27219.222059,6.661287
# gamma=6 specific   5e5 single no-spias:
1000000,35472.458452,17.010340
2000000,35541.241878,14.200865
3000000,35566.933734,11.121769
4000000,35569.977472,13.034212
5000000,35599.083056,12.960910
6000000,35605.684595,19.288496
# gamma=6 specific   1e6 single no-spias:
1000000,50276.879059,69.842187
2000000,50684.641252,60.190155
3000000,50647.743870,67.201960
4000000,50620.347223,64.823658
5000000,50674.701332,76.814125
6000000,50741.200447,111.217853
# gamma=6 specific   2e6 single no-spias:
1000000,74177.851710,233.260873
2000000,78155.170028,254.238761
3000000,78658.786315,210.772457
4000000,79055.510560,165.534292
5000000,79211.361832,148.667113
6000000,79423.942598,208.876369

#                                         0m anneal (insignificantly worse CE distribution if anneal)
#         gamma=1.5
#             anneal=0                -   1.1%  0.3%  0.0%  0.2%  0.0%  -0.1%   0.0%         1.5m non-anneal
#     generic  single  spias
#         gamma=6
#             anneal=0                -  -0.9%  0.6% -0.1% -0.5%  0.5%  -0.5%   0.7%         0.5m non-anneal
#             anneal=250k                      +1.0%                                         500k anneal
#             anneal=500k                      +1.5%
#             anneal=750k                      +0.3%
#             anneal=1m                        +0.7%
#
# ppo1
#     train_num_timesteps        500k   1m      2m      5m     10m       recommended timesteps
#     specific single no-spias     -   1.8%    0.7%    0.3%     ?                ?
#     specific single  spias       -   6.8%    2.0%    1.2%     ?                ?
#     specific couple no-spias     -   5.7%    4.4%    7.6%    3.5%              ?
#     specific couple  spias       -   6.0%    3.9%    6.9%    3.6%              ?
#     generic  single no-spias     -   4.1%    1.2%    0.1%     ?                2m
#     generic  single  spias       -   1.2%    5.9%    1.7%     ?                5m
#     generic  couple no-spias     -  10.9%    6.8%    6.4%    1.0%             10m
#     generic  couple  spias       -  10.0%    1.6%    4.9%    3.6%             10m+
# train_num_timesteps=5000000 IS THUS APPROPRIATE FOR A SPECIFIC INDIVIDUAL MODEL
#
# ppo1 5m/10m (single/couple) timestep generic results as a fraction specific results average of 4 scenarios (with 2m timestep evaluations):
#     single no-spias   98.7%
#     single  spias     95.9%
#     couple no-spias  100.7%
#     couple  spias    100.8%
# THUS IT WOULD APPEAR BETTER TO USE A GENERIC MODEL FOR A COUPLE, BUT TRAIN A SPECIFIC MODEL FOR AN INDIVIDUAL
#
# ppo1 5m/10m (single/couple) timestep no spias results as a fraction of spia results average of 4 scenarios (with 2m timestep evaluations):
#     specific single                   94.4%
#     specific couple  couple-spias    103.4%  5m timesteps; no probabilistic defined benefits
#     specific couple no-couple-spias   99.0%
#     generic  single                   97.2%
#     generic  couple  couple-spias    101.8%  5m timesteps; no probabilistic defined benefits
#     generic  couple no-couple-spias   98.8%
# couple_spias=False IS THUS THE DEFAULT
#
# ppo1 nominal_bonds_duration=15 improvement relative to duration=None average of 4 scenarios
#     (old couple model, no probabilistic defined benefits, 2m timesteps, and 2m timestep evaluation):
#     specific single no-spias    0.2%
#     specific single  spias      0.4%
#     specific couple no-spias    1.5%
#     specific couple  spias      2.3%
#     generic  single no-spias    1.1%
#     generic  single  spias     -0.7%
#     generic  couple no-spias    1.0%
#     generic  couple  spias      1.3%
# nominal_bonds_duration=15 IS THUS THE DEFAULT
#
# ppo1:
#     Specific gamma for variable scenario performs 2-3% worse than specific scenario.
#     Generic gamma for variable scenario perform 2-3% worse than specific gamma for variable scenario.
#
# Notes on ppo1 tweaks that have been tried out (single with no nominal SPIAs):
#
# --train-hidden-layer-size=128
#    Produced 0.2% worse generic results than hidden layer size 64 both pposgd_simple.learn(optim_batchsize=64).
#
# --train-hidden-layer-size=128 --train-single-num-timesteps=2000000
#    Produced 0.6% worse generic results than 1m timesteps and hidden layer size 64 both pposgd_simple.learn(optim_batchsize=64).
#
# pposgd_simple.learn(lam=0.98)
# pposgd_simple.learn(lam=1)
#    Produced 0.2% better and 0.1% worse generic results than lam=0.95 all pposgd_simple.learn(optim_batchsize=64).
#
# pposgd_simple.learn(optim_batchsize=128)
# pposgd_simple.learn(optim_batchsize=256)
#    Produced 0.3% better and 0.3% generic results than optim_batchsize=64.
#    WE NOW DEFAULT TO AN optim_batchsize of 128 COMPARED TO THE ORIGINAL VALUE OF 64.
#
# pposgd_simple.learn(adam_epsilon=1e-6)
#    Produced 0.1% worse generic results than adam_epsilon=1e-5 default both pposgd_simple.learn(optim_batchsize=64).
#
# pposgd_simple.learn(optim_epochs=20)
#    Produced 0.4% worse generic results than optim_epochs=10 both pposgd_simple.learn(optim_batchsize=64).
#
# pposgd_simple.learn(optim_batchsize=128, optim_stepsize=3e-5)
#    Produduced 0.4% worse generic results than optim_batchsize=128, optim_stepsize=3e-4.
#
# pposgd_simple.learn(optim_batchsize=128, optim_stepsize=1e-4)
#    Produduced 0.2% worse generic results than optim_batchsize=128, optim_stepsize=3e-4.
#
# --train-num-hidden-layers=3
#    Produced 0.4% worse generic results than 2 hidden layers both pposgd_simple.learn(optim_batchsize=128)
#
# Notes on PPO1 tweaks that have been tried out (couple with no nominal SPIAs):
#
# --train-num-timesteps=5000000 --eval-consume-clip=5000
#    Produced identical results to eval_consume_clip=0.
#
# Notes on PPO1 tweaks that have been tried out (couple with no nominal SPIAs and nomnal_bonds_duration15):
#
# optim_epochs=50
#    Produced 9.9% worse generic results than optim_epochs=10.
#
# optim_stepsize=1e-4
#    Produced 9.7% worse generic results than optim_stepsize=3e-4.
#
# --train-miniibatch-size=512
#    Produced 11.6% worse generic results than train_minibatch_size=128.
#
# timesteps_per_actor_batch=8192
#    Produced 8.8% worse results than timesteps_per_actor_batch=2048, but with around 1/3 the CE standard deviation.
#
# timesteps_per_actor_batch=8192 --train-num-timesteps=5000000
#    Produced 2.0% worse results than timesteps_per_actor_batch=2048 --train-num-timesteps=2000000, with similar CE standard deviaiton, but no failed runs.
#
# Notes on PPO1 tweaks that have been tried out (single/couple with nominal SPIAs):
#
# --master-nominal-spias --train-single-num-timesteps=2000000 --hidden-layer-size=128
#    Produced 3.0% worse generic results than size 64.
#
# --master-sex2=male --master-nominal-spias --train-num-timesteps=2000000
#    Produced 3.0% worse generic results than --train-single-num-timesteps=2000000 --train-couple-num-timesteps=2000000
#
# --master-sex2=male --master-nominal-spias --train-single-num-timesteps=2000000 --train-couple-num-timesteps=2000000 --no-train-couple-net
#    Produced 10.3% worse generic results than the default, --train-couple-net.
#
# --master-sex2=male --master-nominal-spias --train-single-num-timesteps=2000000 --train-couple-num-timesteps=2000000 --master-gamma=3
#    Produced 8.4% worse generic results than variable gamma.
#
# --master-sex2=male --master-nominal-spias --train-single-num-timesteps=2000000 --train-couple-num-timesteps=2000000 --master-nomal-bonds-duration=15
#    Produced 3.4% better generic results than variable bonds duration.
